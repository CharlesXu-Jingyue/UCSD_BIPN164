{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIPN164 HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jV-NY76HQTwm",
        "qnUzFHLnVZno",
        "hOg55mZZw0GU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: \n",
        "\n",
        "Group number: "
      ],
      "metadata": {
        "id": "4YUC1ufg0TiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions:\n",
        "\n",
        "To start working on this Homework Assignment, create a copy of this notebook and save it within your Google Drive (File -> Save a copy in Drive).\n",
        "\n",
        "This Jupyter notebook contains 3 exercises consisting of coding and discussion questions. For the discussion questions, please write your answer inside a new text cell. Please hand in your .ipynb file (File -> Download -> Download .ipynb) on canvas by 11:59pm on June 1st.\n",
        "\n",
        "To execute the code, it is necessary to load some external packages, so please execute the following code blocks."
      ],
      "metadata": {
        "id": "yo7ZpxYp0Ij2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MMCbQ9oUQQXD"
      },
      "outputs": [],
      "source": [
        "# @title import packages\n",
        "%%capture\n",
        "# for all exercises:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for the hopfield network\n",
        "from scipy import linalg\n",
        "import pickle\n",
        "import gzip\n",
        "from pkg_resources import resource_filename\n",
        "import sys\n",
        "!pip install --upgrade neurodynex3\n",
        "import neurodynex3\n",
        "from neurodynex3.hopfield_network import pattern_tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1 - Perceptron learning"
      ],
      "metadata": {
        "id": "jV-NY76HQTwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0\n",
        "Start by defining a class called \"Perceptron\", which should contain a function called \"predict\" that predicts the labels of the data X it receives as input, and a function called \"fit\" that trains the perceptron based on the data X and the desired labels d.\n",
        "\n",
        "The perceptron classifier is defined by a **weight vector** $W$ (one component for each input dimension) and a **bias** $b$ (a single number). Here we can combine these two into a single vector by concatenation, such that the bias becomes the zeroth component of the array $W$. All components of this vector are initially zero (before learning).\n",
        "\n",
        "Credit: The code below was taken from this [article](https://pythonmachinelearning.pro/perceptrons-the-first-neural-networks/)."
      ],
      "metadata": {
        "id": "6KaFl1ozQsiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "    # The input parameters should be input_size for the number of input neurons, \n",
        "    # lr for the learning rate, and epochs for the number of training epochs.\n",
        "\n",
        "    def __init__(self, input_size, lr=1, epochs=100):\n",
        "        self.W = np.zeros(input_size+1)\n",
        "        # add one for bias\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "    \n",
        "    def activation_fn(self, x):\n",
        "        #return (x >= 0).astype(np.float32)\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Compute the output of the perceptron given an input x and weights W\n",
        "        z = self.W.T.dot(x)\n",
        "        a = self.activation_fn(z)\n",
        "        return a\n",
        "\n",
        "    def fit(self, X, d):\n",
        "        # Train the weights W (including bias) of the perceptron given input data X and desired labels d\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(d.shape[0]):\n",
        "                x = np.insert(X[i], 0, 1)\n",
        "                y = self.predict(x)\n",
        "                e = d[i] - y\n",
        "                self.W = self.W + self.lr * e * x"
      ],
      "metadata": {
        "id": "Xq1nPbxRQs9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define an input data set X and labels d such as to arrange a simple logical AND (four points arranged in a square; positive response only if both inputs are one). \n",
        "\n",
        "* Run the perceptron learning algorithm and print the resulting weight vector. Remember that its zeroth component is the bias."
      ],
      "metadata": {
        "id": "AHYZ497CRme5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "d = np.array([0, 0, 0, 1])\n",
        " \n",
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, d)\n",
        "\n",
        "# Print the weight vector\n",
        "# You code here"
      ],
      "metadata": {
        "id": "qmGnFzORRulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 \n",
        "* Plot the data and the decision boundary corresponding to this weights vector. The different colors correspond to the desired output labels. What do you notice? Are all points classified correctly?\n",
        "\n",
        "* Look at the equation for the straight line we're plotting based on the components of perceptron.W. Can you understand why this is the right equation for the decision boundary?\n"
      ],
      "metadata": {
        "id": "oh7Wab-2R17b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[d==0,0],X[d==0,1],c='red')\n",
        "plt.scatter(X[d==1,0],X[d==1,1],c='blue')\n",
        "\n",
        "plt.plot([-0.5,1.5],[(-perceptron.W[0]+0.5*perceptron.W[1])/perceptron.W[2],(-perceptron.W[0]-1.5*perceptron.W[1])/perceptron.W[2]],'k')\n",
        "plt.xlim([-0.5,1.5])\n",
        "plt.ylim([-0.5,1.5])\n",
        "plt.xlabel(\"First input neuron's activity\")\n",
        "plt.ylabel(\"Second input neuron's activity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LsPUwiyNSLB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2\n",
        "You might encounter a case in which the decision boundary goes through one point, such that its classification is only marginally correct. Our algorithm allows this - it doesn't change W further in this situation. We could modify the algorithm such that the decision boundary always tries to stay away from the data points. Can you imagine how?\n",
        "\n",
        "### 1.3\n",
        "Another, simpler modification would be to keep the same algorithm, but initialize the weight vector to a random non-zero value, such that it becomes very unlikely that the decision boundary would end up passing through a data point. Change the definition of your Perceptron class by initially setting the weight vector W to a random vector of small weights. For this you can use np.random.normal with zero mean and variances equal to say 0.1.\n"
      ],
      "metadata": {
        "id": "TCUO1uxHSVg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the class Perceptron(object) from above and \n",
        "# change it such that the initial weights are random\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "KFl5sqjDSjrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Run the modified perceptron algorithm on the same data for the AND problem. Is the classification boundary now at a finite distance from all the data points? Try this multiple times to see the effect of the random initialization. There are many different weight vectors that solve this problem."
      ],
      "metadata": {
        "id": "xC9VQ9eUSk_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, d)\n",
        "print(perceptron.W)\n",
        "\n",
        "plt.scatter(X[d==0,0],X[d==0,1],c='red')\n",
        "plt.scatter(X[d==1,0],X[d==1,1],c='blue')\n",
        "plt.plot([-0.5,1.5],[(-perceptron.W[0]+0.5*perceptron.W[1])/perceptron.W[2],(-perceptron.W[0]-1.5*perceptron.W[1])/perceptron.W[2]],'k')\n",
        "plt.xlim([-0.5,1.5])\n",
        "plt.ylim([-0.5,1.5])\n",
        "plt.xlabel(\"First input neuron's activity\")\n",
        "plt.ylabel(\"Second input neuron's activity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1tIHps1tSow6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4\n",
        "Now let's try the XOR problem (still four points arranged in a square; positive desired output only if exactly one input is one). Change the values of d to set up such an XOR problem. \n",
        "* Run the algorithm on these inputs and plot the resulting decision boundary. What do you notice? \n",
        "* Would increasing the number of training epochs help solve this problem?"
      ],
      "metadata": {
        "id": "UKjeEFdrSq5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "# Enter the correct labels d below\n",
        "# d = ...\n",
        " \n",
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, d)\n",
        "print(perceptron.W)\n",
        "\n",
        "plt.scatter(X[d==0,0],X[d==0,1],c='red')\n",
        "plt.scatter(X[d==1,0],X[d==1,1],c='blue')\n",
        "plt.plot([-0.5,1.5],[(-perceptron.W[0]+0.5*perceptron.W[1])/perceptron.W[2],(-perceptron.W[0]-1.5*perceptron.W[1])/perceptron.W[2]],'k')\n",
        "plt.xlim([-0.5,1.5])\n",
        "plt.ylim([-0.5,1.5])\n",
        "plt.xlabel(\"First input neuron's activity\")\n",
        "plt.ylabel(\"Second input neuron's activity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jTK2zkUJS6HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5\n",
        "Now let's increase the number of data points. Use the code below to create `P=50` data points sampled from two Gaussian distributions with all variances equal to 0.3. The first 25 point should be sampled from a Gaussian centered on (0.6,0.8), and the second 25 point from a Gaussian centered on (0.3,0.2). The labels should be such that these two groups of points correspond to different classes."
      ],
      "metadata": {
        "id": "27LxuoLvS-WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = 50\n",
        "X = np.zeros((P,2))\n",
        "d = np.zeros(P)\n",
        "M = int(P/2)\n",
        "# print(M)\n",
        "\n",
        "# Fill in the lines below to create the data set as described above\n",
        "# X[:M,0] = np.random.normal(...)\n",
        "# X[:M,1] = np.random.normal(...)\n",
        "# X[M:,0] = np.random.normal(...)\n",
        "# X[M:,1] = np.random.normal(...)\n",
        "# d[:M] = ...\n",
        "\n",
        "# print(X)\n",
        "# print(d)"
      ],
      "metadata": {
        "id": "3l40uavRTLLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run your perceptron algorithm and plot the results. There likely won't be a perfect solution, since these sets of points probably won't be linearly separable. \n",
        "* Does the resulting classifier at least look sensible in that it makes as few mistakes as possible?"
      ],
      "metadata": {
        "id": "nPI5MCoRTR1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, d)\n",
        "print(perceptron.W)\n",
        "\n",
        "plt.scatter(X[d==0,0],X[d==0,1],c='red')\n",
        "plt.scatter(X[d==1,0],X[d==1,1],c='blue')\n",
        "plt.plot([-0.5,1.5],[(-perceptron.W[0]+0.5*perceptron.W[1])/perceptron.W[2],(-perceptron.W[0]-1.5*perceptron.W[1])/perceptron.W[2]],'k')\n",
        "plt.xlim([-0.5,1.5])\n",
        "plt.ylim([-0.5,1.5])\n",
        "plt.xlabel(\"First input neuron's activity\")\n",
        "plt.ylabel(\"Second input neuron's activity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IYNGpbIdTh0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6\n",
        "Try reducing the learning rate `lr` in your perceptron algorithm to a small value, say 0.0001. Does this improve the situation? Learning with smaller learning rate takes longer, so you'll have to make sure to choose a large enough value for the number of training epochs (say 1000).\n",
        "\n"
      ],
      "metadata": {
        "id": "49Lt8Pb9Tmwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Perceptron class from above to create a perceptron with lr=0.0001, epochs=1000\n",
        "# Run the learning algorithm and plot the results as before.\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "C5iU353ITy4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7\n",
        "Test the accuracy of the predictions the classifier by computing the fraction of correctly classified data points `fraction_correct`. Fill in the incomplete line below. \n",
        "* What is the chance level for the fraction correct, i.e. how well could the classifier do by just guessing the label?"
      ],
      "metadata": {
        "id": "bkQfwb0cTxXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iscorrect = np.zeros(P)\n",
        "\n",
        "\n",
        "for i in range(P):\n",
        "    x = np.insert(X[i,:], 0, 1)\n",
        "    y = perceptron.predict(x)\n",
        "    \n",
        "    # Complete the line below\n",
        "    # iscorrect[i] = ...\n",
        "    \n",
        "# print(iscorrect)\n",
        "\n",
        "fraction_correct = sum(iscorrect)/P\n",
        "print(fraction_correct)"
      ],
      "metadata": {
        "id": "2qeyiyX5UABg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8 \n",
        "Above the data was clustered, because it was sampled from two fairly well separated Gaussian distributions. Generate another data set of 50 points sampled from only one Gaussian distribution (say centered on (0.5,0.5) and with variances 0.4) and try to classify it (with labels zero/one for the first/second half of those points). What do you observe?"
      ],
      "metadata": {
        "id": "GiaNORt1UDpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "...\n",
        "\n",
        "\n",
        "# you can use the code below to plot your results\n",
        "# plt.scatter(X[d==0,0],X[d==0,1],c='red')\n",
        "# plt.scatter(X[d==1,0],X[d==1,1],c='blue')\n",
        "# plt.plot([-0.5,1.5],[(-perceptron.W[0]+0.5*perceptron.W[1])/perceptron.W[2],(-perceptron.W[0]-1.5*perceptron.W[1])/perceptron.W[2]],'k')\n",
        "# plt.xlim([-0.5,1.5])\n",
        "# plt.ylim([-0.5,1.5])\n",
        "# plt.xlabel(\"First input neuron's activity\")\n",
        "# plt.ylabel(\"Second input neuron's activity\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "nfq-hCWvUhar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.9\n",
        "The task above was difficult, and very likely not linearly separable, because we had many points in an input space of low dimensionality. Now let's increase the dimensionality of the input space, i.e. add more input neurons. This makes it hard to visualize the data and classifier, but we can still compute the fraction of correct predictions.\n",
        "\n",
        "Generate another set of 50 points sampled from a Gaussian, but now use a 40-dimensional multi-variate distribution instead of a two-dimensional one. This means that the number of input neurons, and therefore the number of weights, is now much larger (N=40).\n",
        "\n",
        "Split the data into two groups by assigning different labels to the first and second halves of the data points. Run the perceptron algorithm and compute the fraction correct. Does your classifier work better now? Do you understand why?\n"
      ],
      "metadata": {
        "id": "EPKzqW03U2Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "# no need to plot the results, just calculate and print the fraction_correct as was done before."
      ],
      "metadata": {
        "id": "9iAYpt3TVE-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.10\n",
        "Slowly increase the number P of data points (always splitting them into two equal groups for classification; using the following values is fine: P $\\in$ {50, 60, 80, 100}) and plot the fraction of correct classifications.\n",
        "* What's the shape of this graph? You may have to run this several times for each value of P and average to reduce the noise. \n",
        "* Can you read off the capacity of your perceptron? \n",
        "* How does this compare to the number of input neurons?"
      ],
      "metadata": {
        "id": "nc5cyu0SVSxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2 - Hopfield Networks"
      ],
      "metadata": {
        "id": "qnUzFHLnVZno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the code below and familiarize yourself with the source code.\n",
        "\n",
        "Credit: The source code and parts of the exercises were made to accompany the book Neuronal Dynamics by Wulfram Gerstner, Werner M. Kistler, Richard Naud and Liam Paninski."
      ],
      "metadata": {
        "id": "xNZdTKg0aeaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Source code\n",
        "class HopfieldNetwork:\n",
        "    \"\"\"Implements a Hopfield network.\n",
        "    Attributes:\n",
        "        nrOfNeurons (int): Number of neurons\n",
        "        weights (numpy.ndarray): nrOfNeurons x nrOfNeurons matrix of weights\n",
        "        state (numpy.ndarray): current network state. matrix of shape (nrOfNeurons, nrOfNeurons)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nr_neurons):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        Args:\n",
        "            nr_neurons (int): Number of neurons. Use a square number to get the\n",
        "            visualizations properly\n",
        "        \"\"\"\n",
        "        # math.sqrt(nr_neurons)\n",
        "        self.nrOfNeurons = nr_neurons\n",
        "        # initialize with random state\n",
        "        self.state = 2 * np.random.randint(0, 2, self.nrOfNeurons) - 1\n",
        "        # initialize random weights\n",
        "        self.weights = 0\n",
        "        self.reset_weights()\n",
        "        self._update_method = _get_sign_update_function()\n",
        "\n",
        "    def reset_weights(self):\n",
        "        \"\"\"\n",
        "        Resets the weights to random values\n",
        "        \"\"\"\n",
        "        self.weights = 1.0 / self.nrOfNeurons * \\\n",
        "            (2 * np.random.rand(self.nrOfNeurons, self.nrOfNeurons) - 1)\n",
        "\n",
        "    def set_dynamics_sign_sync(self):\n",
        "        \"\"\"\n",
        "        sets the update dynamics to the synchronous, deterministic g(h) = sign(h) function\n",
        "        \"\"\"\n",
        "        self._update_method = _get_sign_update_function()\n",
        "\n",
        "    def set_dynamics_sign_async(self):\n",
        "        \"\"\"\n",
        "        Sets the update dynamics to the g(h) =  sign(h) functions. Neurons are updated asynchronously:\n",
        "        In random order, all neurons are updated sequentially\n",
        "        \"\"\"\n",
        "        self._update_method = _get_async_sign_update_function()\n",
        "\n",
        "    def set_dynamics_to_user_function(self, update_function):\n",
        "        \"\"\"\n",
        "        Sets the network dynamics to the given update function\n",
        "        Args:\n",
        "            update_function: upd(state_t0, weights) -> state_t1.\n",
        "                Any function mapping a state s0 to the next state\n",
        "                s1 using a function of s0 and weights.\n",
        "        \"\"\"\n",
        "        self._update_method = update_function\n",
        "\n",
        "    def store_patterns(self, pattern_list):\n",
        "        \"\"\"\n",
        "        Learns the patterns by setting the network weights. The patterns\n",
        "        themselves are not stored, only the weights are updated!\n",
        "        self connections are set to 0.\n",
        "        Args:\n",
        "            pattern_list: a nonempty list of patterns.\n",
        "        \"\"\"\n",
        "        all_same_size_as_net = all(len(p.flatten()) == self.nrOfNeurons for p in pattern_list)\n",
        "        if not all_same_size_as_net:\n",
        "            errMsg = \"Not all patterns in pattern_list have exactly the same number of states \" \\\n",
        "                     \"as this network has neurons n = {0}.\".format(self.nrOfNeurons)\n",
        "            raise ValueError(errMsg)\n",
        "        self.weights = np.zeros((self.nrOfNeurons, self.nrOfNeurons))\n",
        "        # textbook formula to compute the weights:\n",
        "        for p in pattern_list:\n",
        "            p_flat = p.flatten()\n",
        "            for i in range(self.nrOfNeurons):\n",
        "                for k in range(self.nrOfNeurons):\n",
        "                    self.weights[i, k] += p_flat[i] * p_flat[k]\n",
        "        self.weights /= self.nrOfNeurons\n",
        "        # no self connections:\n",
        "        np.fill_diagonal(self.weights, 0)\n",
        "\n",
        "    def set_state_from_pattern(self, pattern):\n",
        "        \"\"\"\n",
        "        Sets the neuron states to the pattern pixel. The pattern is flattened.\n",
        "        Args:\n",
        "            pattern: pattern\n",
        "        \"\"\"\n",
        "        self.state = pattern.copy().flatten()\n",
        "\n",
        "    def iterate(self):\n",
        "        \"\"\"Executes one timestep of the dynamics\"\"\"\n",
        "        self.state = self._update_method(self.state, self.weights)\n",
        "\n",
        "    def run(self, nr_steps=5):\n",
        "        \"\"\"Runs the dynamics.\n",
        "        Args:\n",
        "            nr_steps (float, optional): Timesteps to simulate\n",
        "        \"\"\"\n",
        "        for i in range(nr_steps):\n",
        "            # run a step\n",
        "            self.iterate()\n",
        "\n",
        "    def run_with_monitoring(self, nr_steps=5):\n",
        "        \"\"\"\n",
        "        Iterates at most nr_steps steps. records the network state after every\n",
        "        iteration\n",
        "        Args:\n",
        "            nr_steps:\n",
        "        Returns:\n",
        "            a list of 2d network states\n",
        "        \"\"\"\n",
        "        states = list()\n",
        "        states.append(self.state.copy())\n",
        "        for i in range(nr_steps):\n",
        "            # run a step\n",
        "            self.iterate()\n",
        "            states.append(self.state.copy())\n",
        "        return states\n",
        "\n",
        "\n",
        "def _get_sign_update_function():\n",
        "    \"\"\"\n",
        "    for internal use\n",
        "    Returns:\n",
        "        A function implementing a synchronous state update using sign(h)\n",
        "    \"\"\"\n",
        "    def upd(state_s0, weights):\n",
        "        h = np.sum(weights * state_s0, axis=1)\n",
        "        s1 = np.sign(h)\n",
        "        # by definition, neurons have state +/-1. If the\n",
        "        # sign function returns 0, we set it to +1\n",
        "        idx0 = s1 == 0\n",
        "        s1[idx0] = 1\n",
        "        return s1\n",
        "    return upd\n",
        "\n",
        "\n",
        "def _get_async_sign_update_function():\n",
        "    def upd(state_s0, weights):\n",
        "        random_neuron_idx_list = np.random.permutation(len(state_s0))\n",
        "        state_s1 = state_s0.copy()\n",
        "        for i in range(len(random_neuron_idx_list)):\n",
        "            rand_neuron_i = random_neuron_idx_list[i]\n",
        "            h_i = np.dot(weights[:, rand_neuron_i], state_s1)\n",
        "            s_i = np.sign(h_i)\n",
        "            if s_i == 0:\n",
        "                s_i = 1\n",
        "            state_s1[rand_neuron_i] = s_i\n",
        "        return state_s1\n",
        "    return upd"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W8SW0owkV664"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting functions\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "\n",
        "def plot_pattern(pattern, reference=None, color_map=\"brg\", diff_code=0):\n",
        "    \"\"\"\n",
        "    Plots the pattern. If a (optional) reference pattern is provided, the pattern is  plotted\n",
        "     with differences highlighted\n",
        "    Args:\n",
        "        pattern (numpy.ndarray): N by N pattern to plot\n",
        "        reference (numpy.ndarray):  optional. If set, differences between pattern and reference are highlighted\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    if reference is None:\n",
        "        p = pattern\n",
        "        overlap = 1\n",
        "    else:\n",
        "        p = get_pattern_diff(pattern, reference, diff_code)\n",
        "        overlap = compute_overlap(pattern, reference)\n",
        "\n",
        "    plt.imshow(p, interpolation=\"nearest\", cmap=color_map)\n",
        "    if reference is not None:\n",
        "        plt.title(\"m = {:0.2f}\".format(round(overlap, 2)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_overlap_matrix(overlap_matrix, color_map=\"bwr\"):\n",
        "    \"\"\"\n",
        "    Visualizes the pattern overlap\n",
        "    Args:\n",
        "        overlap_matrix:\n",
        "        color_map:\n",
        "    \"\"\"\n",
        "\n",
        "    plt.imshow(overlap_matrix, interpolation=\"nearest\", cmap=color_map)\n",
        "    plt.title(\"pattern overlap m(i,k)\")\n",
        "    plt.xlabel(\"pattern k\")\n",
        "    plt.ylabel(\"pattern i\")\n",
        "    plt.axes().get_xaxis().set_major_locator(plt.MaxNLocator(integer=True))\n",
        "    plt.axes().get_yaxis().set_major_locator(plt.MaxNLocator(integer=True))\n",
        "    cb = plt.colorbar(ticks=np.arange(-1, 1.01, 0.25).tolist())\n",
        "    cb.set_clim(-1, 1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pattern_list(pattern_list, color_map=\"brg\"):\n",
        "    \"\"\"\n",
        "    Plots the list of patterns\n",
        "    Args:\n",
        "        pattern_list:\n",
        "        color_map:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    f, ax = plt.subplots(1, len(pattern_list))\n",
        "    _plot_list(ax, pattern_list, None, \"P{0}\", color_map)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _plot_list(axes_list, state_sequence, reference=None, title_pattern=\"S({0})\", color_map=\"brg\"):\n",
        "    \"\"\"\n",
        "    For internal use.\n",
        "    Plots all states S(t) or patterns P in state_sequence.\n",
        "    If a (optional) reference pattern is provided, the patters are  plotted with differences highlighted\n",
        "    Args:\n",
        "        state_sequence: (list(numpy.ndarray))\n",
        "        reference: (numpy.ndarray)\n",
        "        title_pattern (str) pattern injecting index i\n",
        "    \"\"\"\n",
        "    for i in range(len(state_sequence)):\n",
        "        if reference is None:\n",
        "            p = state_sequence[i]\n",
        "        else:\n",
        "            p = get_pattern_diff(state_sequence[i], reference, diff_code=-0.2)\n",
        "        axes_list[i].imshow(p, interpolation=\"nearest\", cmap=color_map)\n",
        "        axes_list[i].set_title(title_pattern.format(i))\n",
        "        axes_list[i].axis(\"off\")\n",
        "\n",
        "\n",
        "def plot_state_sequence_and_overlap(state_sequence, pattern_list, reference_idx, color_map=\"brg\", suptitle=None):\n",
        "    \"\"\"\n",
        "    For each time point t ( = index of state_sequence), plots the sequence of states and the overlap (barplot)\n",
        "    between state(t) and each pattern.\n",
        "    Args:\n",
        "        state_sequence: (list(numpy.ndarray))\n",
        "        pattern_list: (list(numpy.ndarray))\n",
        "        reference_idx: (int) identifies the pattern in pattern_list for which wrong pixels are colored.\n",
        "    \"\"\"\n",
        "    if reference_idx is None:\n",
        "        reference_idx = 0\n",
        "    reference = pattern_list[reference_idx]\n",
        "    f, ax = plt.subplots(2, len(state_sequence))\n",
        "    _plot_list(ax[0, :], state_sequence, reference, \"S{0}\", color_map)\n",
        "    for i in range(len(state_sequence)):\n",
        "        overlap_list = compute_overlap_list(state_sequence[i], pattern_list)\n",
        "        ax[1, i].bar(range(len(overlap_list)), overlap_list)\n",
        "        ax[1, i].set_title(\"m = {1}\".format(i, round(overlap_list[reference_idx], 2)))\n",
        "        ax[1, i].set_ylim([-1, 1])\n",
        "        ax[1, i].get_xaxis().set_major_locator(plt.MaxNLocator(integer=True))\n",
        "        if i > 0:  # show lables only for the first subplot\n",
        "            ax[1, i].set_xticklabels([])\n",
        "            ax[1, i].set_yticklabels([])\n",
        "    if suptitle is not None:\n",
        "        f.suptitle(suptitle)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_nework_weights(hopfield_network, color_map=\"jet\"):\n",
        "    \"\"\"\n",
        "    Visualizes the network's weight matrix\n",
        "    Args:\n",
        "        hopfield_network:\n",
        "        color_map:\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(hopfield_network.weights, interpolation=\"nearest\", cmap=color_map)\n",
        "    plt.colorbar()\n",
        "\n",
        "\n",
        "class PatternFactory:\n",
        "    \"\"\"\n",
        "    Creates square patterns of size pattern_length x pattern_width\n",
        "    If pattern length is omitted, square patterns are produced\n",
        "    \"\"\"\n",
        "    def __init__(self, pattern_length, pattern_width=None):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        Args:\n",
        "            pattern_length: the length of a pattern\n",
        "            pattern_width: width or None. If None, patterns are squares of size (pattern_length x pattern_length)\n",
        "        \"\"\"\n",
        "        self.pattern_length = pattern_length\n",
        "        self.pattern_width = pattern_length if pattern_width is None else pattern_width\n",
        "\n",
        "    def create_random_pattern(self, on_probability=0.5):\n",
        "        \"\"\"\n",
        "        Creates a pattern_length by pattern_width 2D random pattern\n",
        "        Args:\n",
        "            on_probability:\n",
        "        Returns:\n",
        "            a new random pattern\n",
        "        \"\"\"\n",
        "        p = np.random.binomial(1, on_probability, self.pattern_length * self.pattern_width)\n",
        "        p = p * 2 - 1  # map {0, 1} to {-1 +1}\n",
        "        return p.reshape((self.pattern_length, self.pattern_width))\n",
        "\n",
        "    def create_random_pattern_list(self, nr_patterns, on_probability=0.5):\n",
        "        \"\"\"\n",
        "        Creates a list of nr_patterns random patterns\n",
        "        Args:\n",
        "            nr_patterns: length of the new list\n",
        "            on_probability:\n",
        "        Returns:\n",
        "            a list of new random patterns of size (pattern_length x pattern_width)\n",
        "        \"\"\"\n",
        "        p = list()\n",
        "        for i in range(nr_patterns):\n",
        "            p.append(self.create_random_pattern(on_probability))\n",
        "        return p\n",
        "\n",
        "    def create_row_patterns(self, nr_patterns=None):\n",
        "        \"\"\"\n",
        "        creates a list of n patterns, the i-th pattern in the list\n",
        "        has all states of the i-th row set to active.\n",
        "        This is convenient to create a list of orthogonal patterns which\n",
        "        are easy to visually identify\n",
        "        Args:\n",
        "            nr_patterns:\n",
        "        Returns:\n",
        "            list of orthogonal patterns\n",
        "        \"\"\"\n",
        "        n = self.pattern_width if nr_patterns is None else nr_patterns\n",
        "        pattern_list = []\n",
        "        for i in range(n):\n",
        "            p = self.create_all_off()\n",
        "            p[i, :] = np.ones((1, self.pattern_length))\n",
        "            pattern_list.append(p)\n",
        "        return pattern_list\n",
        "\n",
        "    def create_all_on(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            2d pattern, all pixels on\n",
        "        \"\"\"\n",
        "        return np.ones((self.pattern_length, self.pattern_width), int)\n",
        "\n",
        "    def create_all_off(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            2d pattern, all pixels off\n",
        "        \"\"\"\n",
        "        return -1 * np.ones((self.pattern_length, self.pattern_width), int)\n",
        "\n",
        "    def create_checkerboard(self):\n",
        "        \"\"\"\n",
        "        creates a checkerboard pattern of size (pattern_length x pattern_width)\n",
        "        Returns:\n",
        "            checkerboard pattern\n",
        "        \"\"\"\n",
        "        pw = np.ones(self.pattern_length, int)\n",
        "        # set every second value to -1\n",
        "        pw[1::2] = -1\n",
        "        pl = np.ones(self.pattern_width, int)\n",
        "        # set every second value to -1\n",
        "        pl[1::2] = -1\n",
        "        t = linalg.toeplitz(pw, pl)\n",
        "        t = t.reshape((self.pattern_length, self.pattern_width))\n",
        "        return t\n",
        "\n",
        "    def create_L_pattern(self, l_width=1):\n",
        "        \"\"\"\n",
        "        creates a pattern with column 0 (left) and row n (bottom) set to +1.\n",
        "        Increase l_width to set more columns and rows (default is 1)\n",
        "        Args:\n",
        "            l_width (int): nr of rows and columns to set\n",
        "        Returns:\n",
        "            an L shaped pattern.\n",
        "        \"\"\"\n",
        "        l_pat = -1 * np.ones((self.pattern_length, self.pattern_width), int)\n",
        "        for i in range(l_width):\n",
        "            l_pat[-i - 1, :] = np.ones(self.pattern_length, int)\n",
        "            l_pat[:, i] = np.ones(self.pattern_length, int)\n",
        "        return l_pat\n",
        "\n",
        "    def reshape_patterns(self, pattern_list):\n",
        "        \"\"\"\n",
        "        reshapes all patterns in pattern_list to have shape = (self.pattern_length, self.pattern_width)\n",
        "        Args:\n",
        "            self:\n",
        "            pattern_list:\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        new_shape = (self.pattern_length, self.pattern_width)\n",
        "        return reshape_patterns(pattern_list, new_shape)\n",
        "\n",
        "\n",
        "def reshape_patterns(pattern_list, shape):\n",
        "    \"\"\"\n",
        "    reshapes each pattern in pattern_list to the given shape\n",
        "    Args:\n",
        "        pattern_list:\n",
        "        shape:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    reshaped_patterns = [p.reshape(shape) for p in pattern_list]\n",
        "    return reshaped_patterns\n",
        "\n",
        "\n",
        "def get_pattern_diff(pattern1, pattern2, diff_code=0):\n",
        "    \"\"\"\n",
        "    Creates a new pattern of same size as the two patterns.\n",
        "    the diff pattern has the values pattern1 = pattern2 where the two patterns have\n",
        "    the same value. Locations that differ between the two patterns are set to\n",
        "    diff_code (default = 0)\n",
        "    Args:\n",
        "        pattern1:\n",
        "        pattern2:\n",
        "        diff_code: the values of the new pattern, at locations that differ between\n",
        "        the two patterns are set to diff_code.\n",
        "    Returns:\n",
        "        the diff pattern.\n",
        "    \"\"\"\n",
        "    if pattern1.shape != pattern2.shape:\n",
        "        raise ValueError(\"patterns are not of equal shape\")\n",
        "    diffs = np.multiply(pattern1, pattern2)\n",
        "    pattern_with_diffs = np.where(diffs < 0, diff_code, pattern1)\n",
        "    return pattern_with_diffs\n",
        "\n",
        "\n",
        "def flip_n(template, nr_of_flips):\n",
        "    \"\"\"\n",
        "    makes a copy of the template pattern and flips\n",
        "    exactly n randomly selected states.\n",
        "    Args:\n",
        "        template:\n",
        "        nr_of_flips:\n",
        "    Returns:\n",
        "        a new pattern\n",
        "    \"\"\"\n",
        "    n = np.prod(template.shape)\n",
        "    # pick nrOfMutations indices (without replacement)\n",
        "    idx_reassignment = np.random.choice(n, nr_of_flips, replace=False)\n",
        "    linear_template = template.flatten()\n",
        "    linear_template[idx_reassignment] = -linear_template[idx_reassignment]\n",
        "    return linear_template.reshape(template.shape)\n",
        "\n",
        "\n",
        "def get_noisy_copy(template, noise_level):\n",
        "    \"\"\"\n",
        "    Creates a copy of the template pattern and reassigns N pixels. N is determined\n",
        "    by the noise_level\n",
        "    Note: reassigning a random value is not the same as flipping the state. This\n",
        "    function reassigns a random value.\n",
        "    Args:\n",
        "        template:\n",
        "        noise_level: a value in [0,1]. for 0, this returns a copy of the template.\n",
        "        for 1, a random pattern of the same size as template is returned.\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if noise_level == 0:\n",
        "        return template.copy()\n",
        "    if noise_level < 0 or noise_level > 1:\n",
        "        raise ValueError(\"noise level is not in [0,1] but {}0\".format(noise_level))\n",
        "    linear_template = template.copy().flatten()\n",
        "    n = np.prod(template.shape)\n",
        "    nr_mutations = int(round(n * noise_level))\n",
        "    idx_reassignment = np.random.choice(n, nr_mutations, replace=False)\n",
        "    rand_values = np.random.binomial(1, 0.5, n)\n",
        "    rand_values = rand_values * 2 - 1  # map {0,1} to {-1, +1}\n",
        "    linear_template[idx_reassignment] = rand_values\n",
        "    return linear_template.reshape(template.shape)\n",
        "\n",
        "\n",
        "def compute_overlap(pattern1, pattern2):\n",
        "    \"\"\"\n",
        "    compute overlap\n",
        "    Args:\n",
        "        pattern1:\n",
        "        pattern2:\n",
        "    Returns: Overlap between pattern1 and pattern2\n",
        "    \"\"\"\n",
        "    shape1 = pattern1.shape\n",
        "    if shape1 != pattern2.shape:\n",
        "        raise ValueError(\"patterns are not of equal shape\")\n",
        "    dot_prod = np.dot(pattern1.flatten(), pattern2.flatten())\n",
        "    return float(dot_prod) / (np.prod(shape1))\n",
        "\n",
        "\n",
        "def compute_overlap_list(reference_pattern, pattern_list):\n",
        "    \"\"\"\n",
        "    Computes the overlap between the reference_pattern and each pattern\n",
        "    in pattern_list\n",
        "    Args:\n",
        "        reference_pattern:\n",
        "        pattern_list: list of patterns\n",
        "    Returns:\n",
        "        A list of the same length as pattern_list\n",
        "    \"\"\"\n",
        "    overlap = np.zeros(len(pattern_list))\n",
        "    for i in range(0, len(pattern_list)):\n",
        "        overlap[i] = compute_overlap(reference_pattern, pattern_list[i])\n",
        "    return overlap\n",
        "\n",
        "\n",
        "def compute_overlap_matrix(pattern_list):\n",
        "    \"\"\"\n",
        "    For each pattern, it computes the overlap to all other patterns.\n",
        "    Args:\n",
        "        pattern_list:\n",
        "    Returns:\n",
        "        the matrix m(i,k) = overlap(pattern_list[i], pattern_list[k]\n",
        "    \"\"\"\n",
        "    nr_patterns = len(pattern_list)\n",
        "    overlap = np.zeros((nr_patterns, nr_patterns))\n",
        "    for i in range(nr_patterns):\n",
        "        for k in range(i, nr_patterns):\n",
        "            if i == k:\n",
        "                overlap[i, i] = 1  # no need to compute the overlap with itself\n",
        "            else:\n",
        "                overlap[i, k] = compute_overlap(pattern_list[i], pattern_list[k])\n",
        "                overlap[k, i] = overlap[i, k]  # because overlap is symmetric\n",
        "    return overlap\n",
        "\n",
        "\n",
        "def load_alphabet():\n",
        "    \"\"\"Load alphabet dict from the file\n",
        "    ``data/alphabet.pickle.gz``, which is included in\n",
        "    the neurodynex release.\n",
        "    Returns:\n",
        "        dict: Dictionary of 10x10 patterns\n",
        "    Raises:\n",
        "        ImportError: Raised if ``neurodynex``\n",
        "            can not be imported. Please install\n",
        "            `neurodynex <pypi.python.org/pypi/neurodynex/>`_.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Todo: consider removing the zip file and explicitly store the strings here.\n",
        "    file_str = \"data/alphabet.pickle.gz\"\n",
        "\n",
        "    try:\n",
        "        file_name = resource_filename(\"neurodynex\", file_str)\n",
        "    except ImportError:\n",
        "        raise ImportError(\n",
        "            \"Could not import data file %s. \" % file_str +\n",
        "            \"Make sure the pypi package `neurodynex` is installed!\"\n",
        "        )\n",
        "\n",
        "    with gzip.open(\"%s\" % file_name) as f:\n",
        "        if sys.version_info < (3, 0, 0):\n",
        "            # python2 pickle.loads has no attribute \"encoding\"\n",
        "            abc_dict = pickle.load(f)\n",
        "        else:\n",
        "            # latin1 is required for python3 compatibility\n",
        "            abc_dict = pickle.load(f, encoding=\"latin1\")\n",
        "\n",
        "    # shape the patterns and provide upper case keys\n",
        "    ABC_dict = dict()\n",
        "    for key in abc_dict:\n",
        "        ABC_dict[key.upper()] = abc_dict[key].reshape((10, 10))\n",
        "    return ABC_dict\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y4ZSgPtXV-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates a small Hopfield network with `nr_neurons = pattern_size ** 2` neurons, and stores a number of patterns (namely nr_patterns, one of which is a checkboard and the rest random). It computes the overlap of these patterns, and then tries to recall one of them from a noisy cue, with noise introduced by flipping the states of nr_of_flips neurons of the originally stored pattern.\n",
        "\n",
        "* Run this code, and try to understand the plots it spits out. Note that even though the patterns are arranged on a square grid, this is purely for visualization purposes. In the Hopfield model all neurons are connected to all others, and there is no spatial structure in the network. "
      ],
      "metadata": {
        "id": "H0W3IwHraiVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_size = 5\n",
        "\n",
        "# create an instance of the class HopfieldNetwork\n",
        "hopfield_net = HopfieldNetwork(nr_neurons= pattern_size**2)\n",
        "# instantiate a pattern factory\n",
        "factory = PatternFactory(pattern_size, pattern_size)\n",
        "# create a checkerboard pattern and add it to the pattern list\n",
        "checkerboard = factory.create_checkerboard()\n",
        "pattern_list = [checkerboard]\n",
        "\n",
        "# add random patterns to the list\n",
        "pattern_list.extend(factory.create_random_pattern_list(nr_patterns=3, on_probability=0.5))\n",
        "plot_pattern_list(pattern_list)\n",
        "# how similar are the random patterns and the checkerboard? Check the overlaps\n",
        "overlap_matrix = compute_overlap_matrix(pattern_list)\n",
        "plot_overlap_matrix(overlap_matrix)\n",
        "\n",
        "# let the hopfield network \"learn\" the patterns. Note: they are not stored\n",
        "# explicitly but only network weights are updated !\n",
        "hopfield_net.store_patterns(pattern_list)\n",
        "\n",
        "# create a noisy version of a pattern and use that to initialize the network\n",
        "noisy_init_state = flip_n(checkerboard, nr_of_flips=4)\n",
        "hopfield_net.set_state_from_pattern(noisy_init_state)\n",
        "\n",
        "# from this initial state, let the network dynamics evolve.\n",
        "states = hopfield_net.run_with_monitoring(nr_steps=4)\n",
        "\n",
        "# each network state is a vector. reshape it to the same shape used to create the patterns.\n",
        "states_as_patterns = factory.reshape_patterns(states)\n",
        "# plot the states of the network\n",
        "plot_state_sequence_and_overlap(states_as_patterns, pattern_list, reference_idx=0, suptitle=\"Network dynamics\")"
      ],
      "metadata": {
        "id": "hWtb9QnqWfd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the code fragment above to answer the following questions. You will need to change some of the parameters, and may want to copy the above into additional code cells below beforehand so that you can compare the outputs.\n"
      ],
      "metadata": {
        "id": "oTmMeOcRazjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - a 4x4 Hopfield network\n",
        "Modify the Python code given above to implement this exercise:\n",
        "\n",
        "* Create a network with $N=16$ neurons.\n",
        "* Create a single 4 by 4 checkerboard pattern.\n",
        "* Store the checkerboard in the network.\n",
        "* Set the initial state of the network to a noisy version of the checkerboard (`nr_flipped_pixels = 5`).\n",
        "* Let the network dynamics evolve for 4 iterations.\n",
        "* Plot the sequence of network states along with the overlap of network state with the checkerboard."
      ],
      "metadata": {
        "id": "cqaFgFoea_eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_size = 4\n",
        "\n",
        "# create an instance of the class HopfieldNetwork\n",
        "hopfield_net = HopfieldNetwork(nr_neurons= 16)\n",
        "# instantiate a pattern factory\n",
        "factory = PatternFactory(pattern_size, pattern_size)\n",
        "# create a checkerboard pattern and add it to the pattern list\n",
        "checkerboard = factory.create_checkerboard()\n",
        "pattern_list = [checkerboard]\n",
        "\n",
        "# add random patterns to the list\n",
        "pattern_list.extend(factory.create_random_pattern_list(nr_patterns=4, on_probability=0.5))\n",
        "plot_pattern_list(pattern_list)\n",
        "# how similar are the random patterns and the checkerboard? Check the overlaps\n",
        "overlap_matrix = compute_overlap_matrix(pattern_list)\n",
        "plot_overlap_matrix(overlap_matrix)\n",
        "\n",
        "# let the hopfield network \"learn\" the patterns. Note: they are not stored\n",
        "# explicitly but only network weights are updated !\n",
        "hopfield_net.store_patterns(pattern_list)\n",
        "\n",
        "# create a noisy version of a pattern and use that to initialize the network\n",
        "noisy_init_state = flip_n(checkerboard, nr_of_flips=10)\n",
        "hopfield_net.set_state_from_pattern(noisy_init_state)\n",
        "\n",
        "# from this initial state, let the network dynamics evolve.\n",
        "states = hopfield_net.run_with_monitoring(nr_steps=4)\n",
        "\n",
        "# each network state is a vector. reshape it to the same shape used to create the patterns.\n",
        "states_as_patterns = factory.reshape_patterns(states)\n",
        "# plot the states of the network\n",
        "plot_state_sequence_and_overlap(states_as_patterns, pattern_list, reference_idx=0, suptitle=\"Network dynamics\")"
      ],
      "metadata": {
        "id": "AWadu5g4bXVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8TUtQSKgpf42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2\n",
        "Test whether the network can still retrieve the pattern if we increase the number of flipped pixels. What happens at `nr_flipped_pixels = 8`, what if nr_flipped_pixels > 8 ?"
      ],
      "metadata": {
        "id": "sdOLCfVKbXob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LFFUsP1sbgus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GbaxjjGVbhHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 - the weight matrix\n",
        "The patterns a Hopfield network learns are not stored explicitly. Instead, the network learns by adjusting the weights to the pattern set it is presented during learning. Let’s visualize this.\n",
        "\n",
        "1. Create a new 4x4 network. Do not yet store any pattern.\n",
        "   What is the size of the network matrix?\n",
        "   Visualize the weight matrix using the function plot_nework_weights(). It takes the network as a parameter.\n",
        "   Create a checkerboard, store it in the network.\n",
        "   Plot the weights matrix. What weight values do occur?\n",
        "2. Create a new 4x4 network.\n",
        "   Create an L-shaped pattern, store it in the network.\n",
        "   Plot the weights matrix. What weight values do occur?\n",
        "3. Create a new 4x4 network.\n",
        "   Create a checkerboard and an L-shaped pattern. Store both patterns in the network.\n",
        "   Plot the weights matrix. What weight values do occur? How does this matrix compare to the two previous matrices?\n",
        "4. For the three networks outlined above, plot the weight distributions. You can easily plot a histogram by adding the following two lines to your script. It assumes you have stored your network in the variable `hopfield_net`.\n",
        "\n",
        "\n",
        "```\n",
        "plt.figure()\n",
        "plt.hist(hopfield_net.weights.flatten())\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bUy_C82wbhjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WKhnfUXacbY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Capacity of an N=100 Hopfield network\n",
        "Larger networks can store more patterns. There is a theoretical limit: the capacity of the Hopfield network. Read [chapter “17.2.4 Memory capacity”](https://neuronaldynamics.epfl.ch/online/Ch17.S2.html) to learn how memory retrieval, pattern completion and the network capacity are related.\n",
        "\n",
        "1. A Hopfield network implements so called associative or content-adressable memory. Explain what this means.\n",
        "\n",
        "2. Using the value $C_{store}$ given in the book, how many patterns can you store in a N=10x10 network? (Use this number $K$ in the next question)\n",
        "\n",
        "3. Create an N=10x10 network and store a checkerboard pattern together with ($K-1$) random patterns. Then initialize the network with the unchanged checkerboard pattern. Let the network evolve for five iterations. \n",
        "Rerun your script a few times. What do you observe?\n"
      ],
      "metadata": {
        "id": "mNOG3bl7cbzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uRFsXgTddB5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5\n",
        "Use the code below to answer the following questions. In case you encounter an error message (ValueError: shape mismatch) you may have set noise_level to zero when using the code below. (This shouldn't affect the exercise much. Bonus points if you can fix this bug such that the code works with a finite noise level during recall.)"
      ],
      "metadata": {
        "id": "e7VsVed_dKJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the letters we want to store in the hopfield network\n",
        "letter_list = ['A', 'B', 'C', 'S', 'X', 'Y', 'Z']\n",
        "\n",
        "# set a seed to reproduce the same noise in the next run\n",
        "# numpy.random.seed(123)\n",
        "\n",
        "abc_dictionary = pattern_tools.load_alphabet()\n",
        "print(\"the alphabet is stored in an object of type: {}\".format(type(abc_dictionary)))\n",
        "# access the first element and get it's size (they are all of same size)\n",
        "pattern_shape = abc_dictionary['A'].shape\n",
        "print(\"letters are patterns of size: {}. Create a network of corresponding size\".format(pattern_shape))\n",
        "# create an instance of the class HopfieldNetwork\n",
        "hopfield_net = HopfieldNetwork(nr_neurons= pattern_shape[0]*pattern_shape[1])\n",
        "\n",
        "# create a list using Pythons List Comprehension syntax:\n",
        "pattern_list = [abc_dictionary[key] for key in letter_list ]\n",
        "plot_pattern_list(pattern_list)\n",
        "\n",
        "# store the patterns\n",
        "hopfield_net.store_patterns(pattern_list)\n",
        "\n",
        "# # create a noisy version of a pattern and use that to initialize the network\n",
        "noisy_init_state = pattern_tools.get_noisy_copy(abc_dictionary['A'], noise_level=0.2)\n",
        "hopfield_net.set_state_from_pattern(noisy_init_state)\n",
        "\n",
        "# from this initial state, let the network dynamics evolve.\n",
        "states = hopfield_net.run_with_monitoring(nr_steps=4)\n",
        "\n",
        "# each network state is a vector. reshape it to the same shape used to create the patterns.\n",
        "states_as_patterns = reshape_patterns(states, pattern_list[0].shape)\n",
        "\n",
        "# plot the states of the network\n",
        "plot_state_sequence_and_overlap(\n",
        "    states_as_patterns, pattern_list, reference_idx=0, suptitle=\"Network dynamics\")\n"
      ],
      "metadata": {
        "id": "SDyCmGzLdbhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add the letter ‘R’ to the letter list and store it in the network. Is the pattern ‘A’ still a fixed point? Does the overlap between the network state and the reference pattern ‘A’ always decrease?\n",
        "\n",
        "2. Make a guess of how many letters the network can store. Then create a (small) set of letters. Check if all letters of your list are fixed points under the network dynamics. Explain the discrepancy between the network capacity $C$ (computed in question 2.4) and your observation."
      ],
      "metadata": {
        "id": "r5VnG3EMgung"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Hebbian Learning\n",
        "\n",
        "The goal of this exercise is to familiarize yourself with basic Hebbian learning rules. \n",
        "\n",
        "## The Basic Hebb Rule\n",
        "A basic plasticity rule that follows the spirit of Hebb's conjecture takes the following form:\n",
        "\n",
        "$$\\tau_w \\frac{d\\mathbf{w}}{dt} = v \\mathbf{u},$$\n",
        "\n",
        "with $\\tau_w$ being the time constant controlling the rate at which the weights change.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "hOg55mZZw0GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Helper Functions\n",
        "def sim_data(n=10000,ratio=1,angle=0):\n",
        "    \"\"\"\n",
        "    DEFINITION\n",
        "    builds an oriented elliptic (or circular) gaussian cloud of 2D points\n",
        "    \n",
        "    INPUT\n",
        "    n: the number of points\n",
        "    ratio: (std along the short axis) / (std along the long axis)\n",
        "    angle: the rotation angle in degrees\n",
        "    \"\"\"\n",
        "    \n",
        "    if ratio>1: ratio=1/ratio\n",
        "    \n",
        "    x = np.random.randn(n,1)\n",
        "    y = ratio * np.random.randn(n,1)\n",
        "    z = np.concatenate((x,y),1)\n",
        "    radangle = (180 - angle) * np.pi / 180\n",
        "    transfo = [[np.cos(radangle),np.sin(radangle)],[-np.sin(radangle), np.cos(radangle)]]\n",
        "    data = np.dot(transfo,z.T).T\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_trace(data_cloud, weights_over_time=None):\n",
        "    \"\"\"\n",
        "    Plots the datapoints and the time series of the weights\n",
        "    Args:\n",
        "        data_cloud (numpy.ndarray): n by 2 data\n",
        "        weights_over_time (numpy.ndarray): n by 2 weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    plt.scatter(\n",
        "        data_cloud[:, 0],\n",
        "        data_cloud[:, 1],\n",
        "        marker=\".\",\n",
        "        facecolor=\"none\",\n",
        "        edgecolor=\"#222222\",\n",
        "        alpha=.2\n",
        "    )\n",
        "    plt.xlabel(\"x1\")\n",
        "    plt.ylabel(\"x2\")\n",
        "\n",
        "    if weights_over_time is not None:\n",
        "      # color time and plot with colorbar\n",
        "      time = np.arange(len(weights_over_time))\n",
        "      colors = plt.cm.cool(time / float(len(time)))\n",
        "      sm = plt.cm.ScalarMappable(\n",
        "          cmap=plt.cm.cool,\n",
        "          norm=plt.Normalize(vmin=0, vmax=len(data_cloud))\n",
        "      )\n",
        "      sm.set_array(time)\n",
        "      cb = plt.colorbar(sm)\n",
        "      cb.set_label(\"Iteration\")\n",
        "      plt.scatter(\n",
        "          weights_over_time[:, 0],\n",
        "          weights_over_time[:, 1],\n",
        "          facecolor=colors,\n",
        "          edgecolor=\"none\",\n",
        "          lw=2\n",
        "      )\n",
        "\n",
        "    # ensure rectangular plot\n",
        "    x_min = data_cloud[:, 0].min()\n",
        "    x_max = data_cloud[:, 0].max()\n",
        "    y_min = data_cloud[:, 1].min()\n",
        "    y_max = data_cloud[:, 1].max()\n",
        "    lims = [min(x_min, y_min), max(x_max, y_max)]\n",
        "    plt.xlim(lims)\n",
        "    plt.ylim(lims)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_weights(weights_course):\n",
        "    \"\"\"\n",
        "    Plots the datapoints and the time series of the weights\n",
        "    Args:\n",
        "        data_cloud (numpy.ndarray): n by 2 data\n",
        "        weights_course (numpy.ndarray): n by 2 weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    plt.ylabel(\"magnitude\")\n",
        "    plt.xlabel(\"time\")\n",
        "\n",
        "    # color time and plot with colorbar\n",
        "    time = np.arange(len(weights_course))\n",
        "    colors = plt.cm.cool(time / float(len(time)))\n",
        "\n",
        "    plt.plot(\n",
        "        time,\n",
        "        weights_course[:, 0],\n",
        "        lw=2\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        time,\n",
        "        weights_course[:, 1],\n",
        "        lw=2\n",
        "    )\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XcwYm3NNxFtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Below* you can find a simple function implementing Hebbian plasticity."
      ],
      "metadata": {
        "id": "kMQIwAnUxJsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Hebb(data, initial_angle=None, lr=0.005):\n",
        "    \"\"\"Run one batch of a simple Hebbian learning rule over\n",
        "    a cloud of datapoints.\n",
        "    Args:\n",
        "        data (numpy.ndarray): An N by 2 array of datapoints. You can\n",
        "            think of each of the two columns as the time series of firing rates of one presynaptic neuron.\n",
        "        initial_angle (float, optional): angle of initial\n",
        "            set of weights [deg]. If None, this is random.\n",
        "        lr (float, optional): learning rate\n",
        "    Returns:\n",
        "        numpy.ndarray: time course of the weight vector\n",
        "    \"\"\"\n",
        "\n",
        "    # get random initial weights\n",
        "    if initial_angle is None:\n",
        "        initial_angle = np.random.rand() * 360.\n",
        "    radangle = initial_angle * np.pi / 180.\n",
        "    w = np.array([np.cos(radangle), np.sin(radangle)])\n",
        "\n",
        "    # save the trajectory of the weights\n",
        "    weights_over_time = np.zeros((len(data), 2), float)\n",
        "    for i in range(0, len(data)):\n",
        "        weights_over_time[i] = w\n",
        "        u = data[i]\n",
        "        v = np.dot(w, u)  # output: postsynaptic firing rate of a linear neuron.\n",
        "        # Hebb rule (data[i] are the two presynaptic firing rates at time point i\n",
        "        w = w + lr * v * u     # update weights\n",
        "\n",
        "    return weights_over_time\n"
      ],
      "metadata": {
        "id": "MOrGEy6uxML8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run it, let's first simulate presynaptic activity over 200 timesteps."
      ],
      "metadata": {
        "id": "iRMQeQKPxP24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = sim_data(n=200, ratio=.3, angle=60)                  # generate random spiking data for our presynaptic neurons\n",
        "plot_trace(data)"
      ],
      "metadata": {
        "id": "MLrysYgTxUdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and simulate a postsynaptic neuron receiving this input using our basic hebbian learning rule."
      ],
      "metadata": {
        "id": "3Ca9_xOLxZCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_over_time = Hebb(data, initial_angle=-20, lr=0.04)  # simulate a postsynaptic neuron using a simple hebbian learning rule\n",
        "plot_weights(weights_over_time)"
      ],
      "metadata": {
        "id": "swMqCTioxco5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the most basic form of Hebb's rule only models synaptic potentiation, the weights for both simulated presynaptic neurons only increase over time.\n"
      ],
      "metadata": {
        "id": "yNIIVij1x-kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 - BCM Rule:\n",
        "Implement the Bienenstock-Cooper-Munro learning rule of equation 8.13 in Dayan & Abbott."
      ],
      "metadata": {
        "id": "AK83ToVhyPv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BCM(data, initial_angle=None, theta=-1, lr1=0.005, lr2 = 0.05):\n",
        "    \"\"\"Run one batch of Bienenstock-Cooper-Munro's learning rule over\n",
        "    a cloud of datapoints.\n",
        "    Args:\n",
        "        data (numpy.ndarray): An N by 2 array of datapoints. You can\n",
        "            think of each of the two columns as the time series of firing rates of one presynaptic neuron.\n",
        "        initial_angle (float, optional): angle of initial\n",
        "            set of weights [deg]. If None, this is random.\n",
        "        lr1 (float, optional): learning rate for the weight vector\n",
        "        lr2 (float, optional): learning rate for theta\n",
        "    Returns:\n",
        "        numpy.ndarray: time course of the weight vector\n",
        "    \"\"\"\n",
        "\n",
        "    # get angle if not set\n",
        "    if initial_angle is None:\n",
        "        initial_angle = np.random.rand() * 360.\n",
        "    radangle = initial_angle * np.pi / 180.\n",
        "\n",
        "    w = np.array([np.cos(radangle), np.sin(radangle)])\n",
        "    weights_over_time = np.zeros((len(data), 2), float)\n",
        "    for i in range(0, len(data)):\n",
        "        weights_over_time[i] = w\n",
        "        u = data[i]\n",
        "        v = ...  # output: postsynaptic firing rate of a linear neuron.\n",
        "        # BCM rule (data[i] are the two presynaptic firing rates at time point i\n",
        "        theta = theta + ...  # update theta\n",
        "        w = w + ...    # update weights\n",
        "\n",
        "    return weights_over_time\n",
        "\n",
        "\n",
        "data = sim_data(n=20000, ratio=..., angle=...)\n",
        "weights_over_time = BCM(data, theta=0, lr1=..., lr2=...)\n",
        "plot_trace(data, weights_over_time)\n",
        "plot_weights(weights_over_time)"
      ],
      "metadata": {
        "id": "dIEEmDyzyTIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the BCM rule. Vary both learning rates. What do you observe? What happens when \n",
        "  * lr1 is larger than lr2?\n",
        "  * lr2 is larger than lr1?\n",
        "  * lr1 is equal to lr2 ?\n",
        "* Plot the time course of the norm of the weights vector\n",
        "* How does the simulation perform if you use data that is not centered at 0?\n",
        "* Next, run the simulation on a data set which has no correlations. Use the functions `sim_data` and `BCM` to get the timecourse for weights that are learned on a circular data cloud (`ratio=1`). Plot the time course of both components of the weight vector. Repeat this many times (BCM will choose random initial conditions on each run), and plot this into the same plot. Can you explain what happens?\n",
        "\n"
      ],
      "metadata": {
        "id": "CeYEkfWIypG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Oja's rule\n",
        "Implement a Hebbian learning rule with multiplicative normalization (Oja's rule)(see equation 8.16 in Dayan & Abbott)."
      ],
      "metadata": {
        "id": "S952MytFysEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oja(data, initial_angle=None, lr=0.005):\n",
        "    \"\"\"Run one batch of Oja's learning over\n",
        "    a cloud of datapoints.\n",
        "    Args:\n",
        "        data (numpy.ndarray): An N by 2 array of datapoints. You can\n",
        "            think of each of the two columns as the time series of firing rates of one presynaptic neuron.\n",
        "        initial_angle (float, optional): angle of initial\n",
        "            set of weights [deg]. If None, this is random.\n",
        "        lr (float, optional): learning rate\n",
        "    Returns:\n",
        "        numpy.ndarray: time course of the weight vector\n",
        "    \"\"\"\n",
        "\n",
        "    # get random initial weights\n",
        "    if initial_angle is None:\n",
        "        initial_angle = np.random.rand() * 360.\n",
        "    radangle = initial_angle * np.pi / 180.\n",
        "\n",
        "    w = np.array([np.cos(radangle), np.sin(radangle)])\n",
        "    weights_over_time = np.zeros((len(data), 2), float)\n",
        "    for i in range(0, len(data)):\n",
        "        weights_over_time[i] = w\n",
        "        u = data[i]  # ojas rule (data[i] are the two presynaptic firing rates at time point i\n",
        "        v = ...  # output: postsynaptic firing rate of a linear neuron.\n",
        "        # ojas rule (data[i] are the two presynaptic firing rates at time point i\n",
        "        w = ...\n",
        "    return weights_over_time\n",
        "\n",
        "data = sim_data(n=20000, ratio=..., angle=...)\n",
        "weights_over_time = oja(data, lr=...)\n",
        "plot_trace(data, weights_over_time)\n",
        "plot_weights(weights_over_time)"
      ],
      "metadata": {
        "id": "O2CyxsVky3Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement Oja's rule. Then modify the script with smaller / larger learning rates. What do you observe?\n",
        "* Plot the time course of the norm of the weights vector.\n",
        "* How does the simulation perform if you use data that is not centered at 0?\n",
        "* Next, run the simulation on a data set which has no correlations. Use the functions `sim_data` and `oja` to get the timecourse for weights that are learned on a circular data cloud (`ratio=1`). Plot the time course of both components of the weight vector. Repeat this many times (oja will choose random initial conditions on each run), and plot this into the same plot. Can you explain what happens?"
      ],
      "metadata": {
        "id": "2eZ1chGzz4yK"
      }
    }
  ]
}